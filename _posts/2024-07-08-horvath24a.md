---
title: 'Maestro: Uncovering Low-Rank Structures via Trainable Decomposition'
openreview: 7bjyambg4x
abstract: Deep Neural Networks (DNNs) have been a large driver for AI breakthroughs
  in recent years, ranging from self-driving cars to intelligent assistants. However,
  these models have been getting increasingly large as they become more accurate and
  safe. This means that their training becomes increasingly costly and time-consuming,
  and typically yields a single model to fit all targets. To mitigate this, various
  techniques have been proposed in the literature, including pruning, sparsification
  or quantization of the model weights and updates. While achieving high compression
  rates, they often incur significant computational overheads at training or lead
  to non-negligible accuracy penalty. Alternatively, factorization methods have been
  leveraged for low-rank compression of DNNs. Similarly, such techniques (e.g., SVD)
  frequently rely on heavy iterative decompositions of layers and are potentially
  sub-optimal for non-linear models, such as DNNs. We take a further step in designing
  efficient low-rank models and propose Maestro, a framework for trainable low-rank
  layers. Instead of iteratively applying a priori decompositions, the low-rank structure
  is baked into the training process through LoD, a low-rank ordered decomposition.
  Not only is this the first time importance ordering via sampling is applied on the
  decomposed DNN structure, but it also allows selecting ranks at a layer granularity.
  Our theoretical analysis demonstrates that LoD recovers the SVD decomposition of
  linear mapping on uniformly distributed data and PCA for linear autoencoders. Applied
  to DNNs, Maestro enables the extraction of lower footprint models that preserve
  performance. Simultaneously, it enables the graceful tradeoff between accuracy-latency
  for deployment to even more constrained devices, without retraining.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: horvath24a
month: 0
tex_title: 'Maestro: Uncovering Low-Rank Structures via Trainable Decomposition'
firstpage: 18859
lastpage: 18881
page: 18859-18881
order: 18859
cycles: false
bibtex_author: Horv\'{a}th, Samuel and Laskaridis, Stefanos and Rajput, Shashank and
  Wang, Hongyi
author:
- given: Samuel
  family: Horv√°th
- given: Stefanos
  family: Laskaridis
- given: Shashank
  family: Rajput
- given: Hongyi
  family: Wang
date: 2024-07-08
address:
container-title: Proceedings of the 41st International Conference on Machine Learning
volume: '235'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 7
  - 8
pdf: https://proceedings.mlr.press/v235/assets/horvath24a/horvath24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
